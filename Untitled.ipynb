{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d188afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Linear algebra\n",
    "# Data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import xml.etree.ElementTree as ET # Reading xml files\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import plot_model\n",
    "from sklearn.manifold import TSNE\n",
    "# For Modelling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, preprocessing, callbacks, optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Add\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.metrics import categorical_accuracy\n",
    "# For Pre-processing\n",
    "import string\n",
    "from string import digits\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "# Other useful modules\n",
    "import h5py\n",
    "from statistics import mode\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "468f261e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"NCT00001328\";\"\n",
      " Malignant brain tumors are responsible for a significant amount of deaths in children and adults. Even with advances in surgery, radiation therapy, and chemotherapy, many patients diagnosed with a malignant brain tumor survive only months to weeks. In an attempt to improve the prognosis for these patients, researchers have developed a new approach to brain tumor therapy. This approach makes use of DNA technology to transfer genes sensitive to therapy into the cells of the tumor. Infections with the herpes simplex virus can cause cold sores in the area of the mouth. A drug called ganciclovir (Cytovene) can kill the virus. Ganciclovir is effective because the herpes virus contains a gene (Herpes-Thymidine Kinase TK gene) that is sensitive to the drug. Researchers have been able to separate this gene from the virus. Using DNA technology, researchers hope to transfer and implant the TK gene into tumor cells making them sensitive to ganciclovir. In theory, giving patients ganciclovir will kill all tumor cells that have the TK gene incorporated into them. \";\"Gene Therapy for the Treatment of Brain Tumors\";\"\";\"August 21, 1992\";\"April 30, 2010\"\n"
     ]
    }
   ],
   "source": [
    "def csv_row(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    nct_text = \"\"\n",
    "    sum_text = \"\"\n",
    "    model_text = \"\"\n",
    "    ph_text = \"\"\n",
    "    title_text = \"\"\n",
    "    start_date = \"\"\n",
    "    completion_date = \"\"\n",
    "    # Only iterates through Phase 2 and 3 studies\n",
    "    for ph in root.iter('phase'):\n",
    "        ph_text = ph.text\n",
    "        if (ph_text == \"Phase 2\" or ph_text == \"Phase 3\" or ph_text == \"Phase 1\"):\n",
    "            #This bit finds all roots with nct_id which is a sub_root to id_info\n",
    "            for nct in root.findall('id_info'):\n",
    "                nctId_text = nct.find('nct_id').text\n",
    "                nct_text =nctId_text\n",
    "            # This bit finds the brief summary text\n",
    "            for s in root.findall('brief_summary'):\n",
    "                summary_text = s.find('textblock').text\n",
    "                sum_text= summary_text\n",
    "                sum_text = sum_text.replace('\\r\\n', '') # Replaces newline with a whitespace\n",
    "                sum_text = re.sub(' +',' ',sum_text) # Compresses multiple whitespaces to only one\n",
    "                #print(\"Summary Text:\", sum_text)\n",
    "            # Get's the official title for the study\n",
    "            for t in root.iter('brief_title'):\n",
    "                title_text = t.text\n",
    "            # This get's the type of intervention_model\n",
    "            for y in root.iter('intervention_model'):\n",
    "                model_text = y.text\n",
    "            if(root.find('start_date') == None):\n",
    "                return None \n",
    "            for s in root.iter('start_date'):\n",
    "                start_date = s.text\n",
    "            if(root.find('primary_completion_date') == None):\n",
    "                return None    \n",
    "            for c in root.iter('primary_completion_date'):\n",
    "                completion_date = c.text\n",
    "    total_text = \"\\\"\" + nct_text + \"\\\"\" + \";\" + \"\\\"\" + sum_text + \"\\\"\" + \";\"  + \"\\\"\" + title_text + \"\\\"\" + \";\"  +  \"\\\"\" + model_text + \"\\\"\"+ \";\"  + \"\\\"\" + start_date + \"\\\"\"+ \";\"  + \"\\\"\" + completion_date + \"\\\"\"\n",
    "    # This functions returns a text with Nct_Id, brief_summary, title and type of intervention model on the form we intended\n",
    "    return total_text\n",
    "print(csv_row(\"Downloads\\\\search_result\\\\NCT00001328.xml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6aa9053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = \"Downloads\\\\search_result\"# Folders in directory where the all the xml files are placed\n",
    "with open('train_data.csv', 'w', encoding=\"utf-8\") as csvfile: \n",
    "    for root, dirs, files in os.walk(rdir):\n",
    "        for filename in files:\n",
    "            name = os.path.join(root, filename)\n",
    "            data = csv_row(name)\n",
    "            if(data):\n",
    "                csvfile.write(data) #Writes total_text into a row in to train_data.csv\n",
    "                csvfile.write(\"\\n\") # Skips to next line and do the same\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5209cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Nct_id                                            Summary  \\\n",
      "3     NCT00001806   In 1997, the Genetics Department of the NCI M...   \n",
      "5     NCT00002463   RATIONALE: Drugs used in chemotherapy use dif...   \n",
      "10    NCT00002484   RATIONALE: Radiation therapy uses high-energy...   \n",
      "22    NCT00002527   RATIONALE: Chemoprevention therapy is the use...   \n",
      "23    NCT00002528   RATIONALE: Removing axillary lymph nodes may ...   \n",
      "...           ...                                                ...   \n",
      "9970  NCT05520372   This is a retrospective study that included 6...   \n",
      "9972  NCT05536362   The Study Showed that combining clonidine and...   \n",
      "9973  NCT05590650   The goal of this clinical trial is to learn a...   \n",
      "9974  NCT05591456   Accelerated hypofractionated 1 week post-mast...   \n",
      "9975  NCT05622357   The goal of this clinical trial is to investi...   \n",
      "\n",
      "                                                  Title  \\\n",
      "3       Methods in Education for Breast Cancer Genetics   \n",
      "5     Combination Chemotherapy in Treating Children ...   \n",
      "10    Radiation Therapy Using 3-Dimensional Treatmen...   \n",
      "22    Aspirin in Treating Patients With Colorectal C...   \n",
      "23    Surgery With or Without Lymph Node Removal in ...   \n",
      "...                                                 ...   \n",
      "9970  Autologous Immune Enhancement Therapy (AIET) f...   \n",
      "9972  Propofol With Clonidine and Ketamine Throughou...   \n",
      "9973  A Pilot Study of Additional Chinese Formula fo...   \n",
      "9974  Accelerated Hypofractionated 1 Week Radiothera...   \n",
      "9975  Prospective Study of Short Course Radiation Th...   \n",
      "\n",
      "                        Model  Duration        Start Date           End Date  \n",
      "3         Parallel Assignment      6810     April 6, 1999   December 6, 2017  \n",
      "5     Single Group Assignment      6540     February 1989       January 2007  \n",
      "10    Single Group Assignment      6115      October 1991          July 2008  \n",
      "22        Parallel Assignment      3590          May 1993         March 2003  \n",
      "23        Parallel Assignment      3495          May 1993      December 2002  \n",
      "...                       ...       ...               ...                ...  \n",
      "9970  Single Group Assignment      2184   January 1, 2016  December 30, 2021  \n",
      "9972      Parallel Assignment       335  February 1, 2021    January 1, 2022  \n",
      "9973  Single Group Assignment       707      July 7, 2018      June 14, 2020  \n",
      "9974      Parallel Assignment       544   January 1, 2018      June 30, 2019  \n",
      "9975  Single Group Assignment      1002    March 18, 2018  December 20, 2020  \n",
      "\n",
      "[8104 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Earlier we saw that the returned text from our function was seperated by ';', so we use this as seperator when reading in the files\n",
    "df = pd.read_csv(\"train_data.csv\", sep=';', header=None,error_bad_lines=False, warn_bad_lines=False)\n",
    "# Give the data sets appropiate column names\n",
    "df.columns =  ['Nct_id', 'Summary', 'Title','Model','Start Date','End Date']\n",
    "# We drop all the observations containing NaN's (missing values)\n",
    "train = df.dropna()\n",
    "duration = []\n",
    "monthToNum = {\"January\": 1, \"February\":2, \"March\":3, \"April\":4, \"May\":5, \"June\":6, \"July\":7, \"August\":8, \"September\":9, \"October\":10, \"November\":11, \"December\":12}\n",
    "for i,row in train.iterrows():\n",
    "    if(\"\\r\\n\" in row['Summary']):\n",
    "        row['Summary'] = row['Summary'].replace(\"\\r\\n\", '')\n",
    "\n",
    "    sd = row['Start Date'].split(' ')\n",
    "    if(len(sd) == 2):\n",
    "        sdl = monthToNum[sd[0]]*30 + 365*int(sd[1]) + 1\n",
    "    else:\n",
    "        sdl = monthToNum[sd[0]]*30 + int(sd[1].replace(\",\", \"\")) + int(sd[2])*365\n",
    "    ed = row['End Date'].split(' ')\n",
    "    if(len(ed) == 2):\n",
    "        edl = monthToNum[ed[0]]*30 + 365*int(ed[1]) + 1\n",
    "    else:\n",
    "        edl = monthToNum[ed[0]]*30 + int(ed[1].replace(\",\", \"\")) + int(ed[2])*365\n",
    "    duration.append(edl - sdl)\n",
    "train.insert(4,\"Duration\", duration)\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e51661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['Model','Start Date', 'End Date'], axis=1)\n",
    "Y = train.drop(['Nct_id','Summary','Title', 'Start Date', 'End Date','Model'], axis = 1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa566731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Duration\n",
      "7444      3075\n",
      "3181      2525\n",
      "7252      1957\n",
      "2838       670\n",
      "7518       855\n",
      "(6483, 4)\n",
      "(6483, 1)\n",
      "(1621, 4)\n",
      "(1621, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.head()) # Prints the first 5 rows of the data\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2af1a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Duration\n",
      "0.25     760.0\n",
      "0.50    1174.0\n",
      "0.75    1790.0\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.quantile([0.25,0.5,0.75]))\n",
    "for i in Y_train:\n",
    "    if(i < )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabd6bb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17704\\3071113660.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# One Hot Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_dummy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_dummy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dummy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train shape:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_dummy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_train' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0bbf1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6928f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be download for the lemmatization (converting to base form)\n",
    "def text_cleaner(dataframe_org):\n",
    "    dataframe = dataframe_org.copy()\n",
    "    columns = ['Summary', 'Title']\n",
    "    for col in columns:\n",
    "        dataframe[col] = dataframe[col].str.translate(str.maketrans(' ', ' ', string.punctuation)) # Remove punctuation\n",
    "        dataframe[col] = dataframe[col].str.translate(str.maketrans(' ', ' ', '\\n')) # Remove newlines\n",
    "        dataframe[col] =dataframe[col].str.translate(str.maketrans(' ', ' ', digits)) # Remove digits\n",
    "        dataframe[col] =dataframe[col].apply(lambda tweet: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',tweet)) # Split combined words\n",
    "        dataframe[col] =dataframe[col].str.lower() # Convert to lowercase\n",
    "        dataframe[col] =dataframe[col].str.split() # Split each sentence using delimiter\n",
    "    # This part is for converting to base form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sum_l=[]\n",
    "    tit_l = []\n",
    "    for y in tqdm(dataframe[columns[0]]): # tqdm is just a progress bar, an this loop only looks at summaries\n",
    "        sum_new=[]\n",
    "        for x in y: # Looks at words in every summary text\n",
    "            z=lemmatizer.lemmatize(x)\n",
    "            z=lemmatizer.lemmatize(z,'v') # The v specifies that it is in doubt of example a word is a noun or verb, it would consider it a verb.\n",
    "            sum_new.append(z)\n",
    "        y = sum_new\n",
    "        sum_l.append(y)\n",
    "    for w in tqdm(dataframe[columns[1]]): # Looks at titles\n",
    "        tit_new=[]\n",
    "        for x in w: # Every word in the titles\n",
    "            z=lemmatizer.lemmatize(x)\n",
    "            z=lemmatizer.lemmatize(z,'v')\n",
    "            tit_new.append(z)\n",
    "        w = tit_new\n",
    "        tit_l.append(w)\n",
    "    # This will join the words into strings as in the original data, just pre-processed and put into list\n",
    "    sum_l2 = []\n",
    "    for col in sum_l:\n",
    "        col = ' '.join(col)\n",
    "        sum_l2.append(col)\n",
    "    tit_l2 = []\n",
    "    for col in tit_l:\n",
    "        col = ' '.join(col)\n",
    "        tit_l2.append(col)\n",
    "    # Data obtained after Lemmatization is in array form, and is converted to Dataframe in the next step.\n",
    "    sum_data=pd.DataFrame(np.array(sum_l2), index=dataframe.index,columns={columns[0]})\n",
    "    tit_data=pd.DataFrame(np.array(tit_l2), index=dataframe.index,columns={columns[1]})\n",
    "    frames = [sum_data, tit_data]\n",
    "    merged = pd.concat(frames, axis=1)\n",
    "    return merged\n",
    "def create_tok(train_data, MAX_FEATURES):\n",
    "    clean_data = text_cleaner(train_data)\n",
    "    tokenizer_sum = text.Tokenizer(num_words=MAX_FEATURES) # Keep the 20.000 most frequent words\n",
    "    tokenizer_tit =  text.Tokenizer(num_words=MAX_FEATURES)\n",
    "    # Summary Text\n",
    "    summary_list = clean_data['Summary']\n",
    "    tokenizer_sum.fit_on_texts(list(summary_list)) # Builds the word index\n",
    "    #Title Text\n",
    "    title_list = clean_data['Title'] # Text from Title\n",
    "    tokenizer_tit.fit_on_texts(list(title_list))\n",
    "    return tokenizer_sum, tokenizer_tit\n",
    "def pre_process(dataframe, tokenizer, col, MAXLEN):\n",
    "    clean_data = text_cleaner(dataframe)\n",
    "    tokenized_list = tokenizer.texts_to_sequences(clean_data[col])\n",
    "    X = sequence.pad_sequences(tokenized_list, maxlen=MAXLEN)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5dc78fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6483/6483 [00:02<00:00, 2605.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6483/6483 [00:00<00:00, 13006.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6483/6483 [00:02<00:00, 2585.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6483/6483 [00:00<00:00, 14002.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6483/6483 [00:02<00:00, 2740.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6483/6483 [00:01<00:00, 5785.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  34  36   9]\n",
      " [  0   0   0 ...   1  10  27]\n",
      " [  0   0   0 ... 310  36   9]\n",
      " ...\n",
      " [  0   0   0 ...  89   2  23]\n",
      " [  0   0   0 ... 237  36   9]\n",
      " [  0   0   0 ... 473 156   9]]\n",
      "[[  0   0   0 ...  11  13   3]\n",
      " [  0   0   0 ...   2  18  15]\n",
      " [  0   0   0 ... 284  13   3]\n",
      " ...\n",
      " [  0   0   0 ...  52  18  15]\n",
      " [  0   0   0 ...  91  13   3]\n",
      " [  0   0   0 ...  14  43   3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1621/1621 [00:00<00:00, 2262.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1621/1621 [00:00<00:00, 11638.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1621/1621 [00:00<00:00, 2347.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1621/1621 [00:00<00:00, 11357.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Duration\n",
      "7444      3075\n",
      "3181      2525\n",
      "7252      1957\n",
      "2838       670\n",
      "7518       855\n",
      "...        ...\n",
      "6851      1095\n",
      "4192      2529\n",
      "8593      1260\n",
      "1971       455\n",
      "6915      2340\n",
      "\n",
      "[6483 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 20000 # Size of vocabluary\n",
    "MAXLEN = 220 # Size of each text sequence, you can tune this depending on the mean length of you text sequences\n",
    "tok_sum, tok_tit = create_tok(X_train,MAX_FEATURES )\n",
    "# The following are used for model.fit\n",
    "X_sum = pre_process(X_train, tok_sum, 'Summary', MAXLEN)\n",
    "X_tit = pre_process(X_train, tok_tit, 'Title', MAXLEN)\n",
    "print(X_sum)\n",
    "print(X_tit)\n",
    "# This is used for prediction\n",
    "X_sum_test = pre_process(X_test, tok_sum, 'Summary', MAXLEN)\n",
    "X_tit_test = pre_process(X_test, tok_tit, 'Title', MAXLEN)\n",
    "list_classes = [\"Model_Crossover Assignment\", \"Model_Other\", \"Model_Parallel Assignment\", \"Model_Single Group Assignment\"] # The 4 categories\n",
    "y = Y_train\n",
    "print(Y_train)\n",
    "# y_test is used for model.evaluate later on\n",
    "y_test = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c91ed567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 220)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 220)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 440)          0           ['input_9[0][0]',                \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 440, 50)      1000000     ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 50)           20200       ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 50)           0           ['lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 50)           2550        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 50)           0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 50)          200         ['dropout_9[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            51          ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,023,001\n",
      "Trainable params: 1,022,901\n",
      "Non-trainable params: 100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_con_model():\n",
    "    embed_size = 50 # How big  each word vector should be\n",
    "    inp_sum = Input(shape=(MAXLEN, ))\n",
    "    inp_title = Input(shape=(MAXLEN, ))\n",
    "    total_inp = concatenate([inp_sum, inp_title]) # Merge the 2 inputs\n",
    "    embed_layer = Embedding(MAX_FEATURES, embed_size)(total_inp)\n",
    "    lstm_layer = LSTM(50)(embed_layer)\n",
    "    layer1 = Dropout(0.1)(lstm_layer) # Regularization method, has the effect of reducing overfitting\n",
    "    layer2 = Dense(50, activation=\"relu\")(layer1) # The relu function can return very large values\n",
    "    layer3 =  Dropout(0.1)(layer2) # Again regularization\n",
    "    layer4 =BatchNormalization()(layer3) # Maintains the mean activation close to 0 and the activation standard deviation close to 1\n",
    "    layer5 = Dense(1, activation=\"softmax\")(layer4) # Only outputs values between 0 and 1, this is the final layer\n",
    "    model_con = Model(inputs=[inp_sum,inp_title], outputs=layer5)\n",
    "    model_con.compile(loss='categorical_crossentropy', # This is the loss function, and this type of function is used when solving categorical classification\n",
    "                    optimizer='rmsprop', # Algorithm that update network weights iterative based in training data\n",
    "                    metrics=[\"accuracy\"]) # This is our statistical measure\n",
    "    return model_con\n",
    "con_model = get_con_model()\n",
    "# Gets informations about the layers in the model, including output, input and number of parameters:\n",
    "con_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4fcf8f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to weights_base.hdf5\n",
      "183/183 - 22s - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - 22s/epoch - 119ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.00000\n",
      "183/183 - 18s - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - 18s/epoch - 98ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.00000\n",
      "183/183 - 22s - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - 22s/epoch - 118ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00000\n",
      "183/183 - 29s - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - 29s/epoch - 156ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # number of samples that will be propagated through the network.\n",
    "epochs = 10 # Number of passes over the entire data set\n",
    "file_path=\"weights_base.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min') # Verbose means that it prints acc and loss\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3) \n",
    "#EarlyStopping should only be includede when tuning your model\n",
    "callbacks_list = [checkpoint, early]\n",
    "history = con_model.fit([X_sum, X_tit], y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list, verbose=2) # Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597f668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
