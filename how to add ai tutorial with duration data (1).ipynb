{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92dc752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Linear algebra\n",
    "# Data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import xml.etree.ElementTree as ET # Reading xml files\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import plot_model\n",
    "from sklearn.manifold import TSNE\n",
    "# For Modelling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, preprocessing, callbacks, optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Add\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.metrics import categorical_accuracy\n",
    "# For Pre-processing\n",
    "import string\n",
    "from string import digits\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "# Other useful modules\n",
    "import h5py\n",
    "from statistics import mode\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3686f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"NCT00005606\";\"\n",
      " RATIONALE: Peripheral blood lymphocyte therapy may be effective in the treatment and prevention of Epstein-Barr virus infection following transplantation. PURPOSE: Phase II trial to study the effectiveness of peripheral blood lymphocyte therapy in treating and preventing lymphoproliferative disorders in patients who have Epstein-Barr virus infection following transplantation. \";\"Peripheral Blood Lymphocyte Therapy to Prevent Lymphoproliferative Disorders Caused by Epstein-Barr Virus in Patients Who Have Undergone Transplantation\";\"\";\"February 2000\";\"September 2003\"\n"
     ]
    }
   ],
   "source": [
    "def csv_row(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    nct_text = \"\"\n",
    "    sum_text = \"\"\n",
    "    model_text = \"\"\n",
    "    ph_text = \"\"\n",
    "    title_text = \"\"\n",
    "    start_date = \"\"\n",
    "    completion_date = \"\"\n",
    "    # Only iterates through Phase 2 and 3 studies\n",
    "    for ph in root.iter('phase'):\n",
    "        ph_text = ph.text\n",
    "        if (ph_text == \"Phase 2\" or ph_text == \"Phase 3\" or ph_text == \"Phase 1\"):\n",
    "            #This bit finds all roots with nct_id which is a sub_root to id_info\n",
    "            for nct in root.findall('id_info'):\n",
    "                nctId_text = nct.find('nct_id').text\n",
    "                nct_text =nctId_text\n",
    "            # This bit finds the brief summary text\n",
    "            for s in root.findall('brief_summary'):\n",
    "                summary_text = s.find('textblock').text\n",
    "                sum_text= summary_text\n",
    "                sum_text = sum_text.replace('\\r\\n', '') # Replaces newline with a whitespace\n",
    "                sum_text = re.sub(' +',' ',sum_text) # Compresses multiple whitespaces to only one\n",
    "                #print(\"Summary Text:\", sum_text)\n",
    "            # Get's the official title for the study\n",
    "            for t in root.iter('brief_title'):\n",
    "                title_text = t.text\n",
    "            # This get's the type of intervention_model\n",
    "            for y in root.iter('intervention_model'):\n",
    "                model_text = y.text\n",
    "            if(root.find('start_date') == None):\n",
    "                return None \n",
    "            for s in root.iter('start_date'):\n",
    "                start_date = s.text\n",
    "            if(root.find('primary_completion_date') == None):\n",
    "                return None    \n",
    "            for c in root.iter('primary_completion_date'):\n",
    "                completion_date = c.text\n",
    "    total_text = \"\\\"\" + nct_text + \"\\\"\" + \";\" + \"\\\"\" + sum_text + \"\\\"\" + \";\"  + \"\\\"\" + title_text + \"\\\"\" + \";\"  +  \"\\\"\" + model_text + \"\\\"\"+ \";\"  + \"\\\"\" + start_date + \"\\\"\"+ \";\"  + \"\\\"\" + completion_date + \"\\\"\"\n",
    "    # This functions returns a text with Nct_Id, brief_summary, title and type of intervention model on the form we intended\n",
    "    return total_text\n",
    "print(csv_row(\"Downloads\\\\search_result (4)\\\\NCT00005606.xml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ae0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = \"Downloads\\\\search_result (4)\"# Folders in directory where the all the xml files are placed\n",
    "with open('train_data.csv', 'w', encoding=\"utf-8\") as csvfile: \n",
    "    for root, dirs, files in os.walk(rdir):\n",
    "        for filename in files:\n",
    "            name = os.path.join(root, filename)\n",
    "            data = csv_row(name)\n",
    "            if(data):\n",
    "                csvfile.write(data) #Writes total_text into a row in to train_data.csv\n",
    "                csvfile.write(\"\\n\") # Skips to next line and do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f63618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Nct_id                                            Summary  \\\n",
      "0     NCT00000479   The purpose of this study is to evaluate the ...   \n",
      "1     NCT00001277   Observational Phase: Patients whose parathyro...   \n",
      "4     NCT00001566   This is a single arm study. The tumor specime...   \n",
      "5     NCT00001637   Diseases such as leukemia, lymphoma, and mult...   \n",
      "6     NCT00001806   In 1997, the Genetics Department of the NCI M...   \n",
      "...           ...                                                ...   \n",
      "9967  NCT05502458   To investigate the effects of perioperative a...   \n",
      "9969  NCT05553808   This study is a sub-study of the master proto...   \n",
      "9970  NCT05591456   Accelerated hypofractionated 1 week post-mast...   \n",
      "9971  NCT05614518   The goal of this clinical trial was to assess...   \n",
      "9972  NCT05622357   The goal of this clinical trial is to investi...   \n",
      "\n",
      "                                                  Title  \\\n",
      "0     Women's Health Study (WHS): A Randomized Trial...   \n",
      "1              Studies of Elevated Parathyroid Activity   \n",
      "4     A Pilot Study of Autologous T-Cell Transplanta...   \n",
      "5     Immunosuppressive Preparation Followed by Bloo...   \n",
      "6       Methods in Education for Breast Cancer Genetics   \n",
      "...                                                 ...   \n",
      "9967  Effect of Propofol and Desflurane on Nucleic A...   \n",
      "9969  Platform Trial of Novel Regimens Versus Standa...   \n",
      "9970  Accelerated Hypofractionated 1 Week Radiothera...   \n",
      "9971  Diagnostic Performance and Safety of 18F-NaF-P...   \n",
      "9972  Prospective Study of Short Course Radiation Th...   \n",
      "\n",
      "                        Model  Duration          Start Date  \\\n",
      "0        Factorial Assignment      4200      September 1992   \n",
      "1         Parallel Assignment      9581   December 15, 1993   \n",
      "4     Single Group Assignment      4290       December 1996   \n",
      "5     Single Group Assignment      3954  September 29, 1997   \n",
      "6         Parallel Assignment      6810       April 6, 1999   \n",
      "...                       ...       ...                 ...   \n",
      "9967      Parallel Assignment       617    December 1, 2020   \n",
      "9969      Parallel Assignment       969    January 24, 2019   \n",
      "9970      Parallel Assignment       544     January 1, 2018   \n",
      "9971     Crossover Assignment       428      March 29, 2021   \n",
      "9972  Single Group Assignment      1002      March 18, 2018   \n",
      "\n",
      "                End Date  \n",
      "0             March 2004  \n",
      "1         March 11, 2020  \n",
      "4         September 2008  \n",
      "5          July 28, 2008  \n",
      "6       December 6, 2017  \n",
      "...                  ...  \n",
      "9967      August 8, 2022  \n",
      "9969  September 23, 2021  \n",
      "9970       June 30, 2019  \n",
      "9971        June 2, 2022  \n",
      "9972   December 20, 2020  \n",
      "\n",
      "[7672 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Earlier we saw that the returned text from our function was seperated by ';', so we use this as seperator when reading in the files\n",
    "df = pd.read_csv(\"train_data.csv\", sep=';', header=None,error_bad_lines=False, warn_bad_lines=False)\n",
    "# Give the data sets appropiate column names\n",
    "df.columns =  ['Nct_id', 'Summary', 'Title','Model','Start Date','End Date']\n",
    "# We drop all the observations containing NaN's (missing values)\n",
    "train = df.dropna()\n",
    "duration = []\n",
    "monthToNum = {\"January\": 1, \"February\":2, \"March\":3, \"April\":4, \"May\":5, \"June\":6, \"July\":7, \"August\":8, \"September\":9, \"October\":10, \"November\":11, \"December\":12}\n",
    "for i,row in train.iterrows():\n",
    "    if(\"\\r\\n\" in row['Summary']):\n",
    "        row['Summary'] = row['Summary'].replace(\"\\r\\n\", '')\n",
    "\n",
    "    sd = row['Start Date'].split(' ')\n",
    "    if(len(sd) == 2):\n",
    "        sdl = monthToNum[sd[0]]*30 + 365*int(sd[1]) + 1\n",
    "    else:\n",
    "        sdl = monthToNum[sd[0]]*30 + int(sd[1].replace(\",\", \"\")) + int(sd[2])*365\n",
    "    ed = row['End Date'].split(' ')\n",
    "    if(len(ed) == 2):\n",
    "        edl = monthToNum[ed[0]]*30 + 365*int(ed[1]) + 1\n",
    "    else:\n",
    "        edl = monthToNum[ed[0]]*30 + int(ed[1].replace(\",\", \"\")) + int(ed[2])*365\n",
    "    duration.append(edl - sdl)\n",
    "train.insert(4,\"Duration\", duration)\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5b88fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Duration  short  Relatively Short  Relatively Long  Long\n",
      "0.25     797.0    0.0               0.0              0.0   0.0\n",
      "0.50    1245.0    0.0               0.0              0.0   0.0\n",
      "0.75    1915.0    0.0               0.0              1.0   1.0\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding for the duration results\n",
    "print(Y_train.quantile([0.25,0.5,0.75]))\n",
    "short = []\n",
    "mediumShort = []\n",
    "mediumLong = []\n",
    "long = []\n",
    "\n",
    "for i,row in train.iterrows():\n",
    "    duration = row['Duration']\n",
    "    if(duration < 794):\n",
    "        short.append(1)\n",
    "        mediumShort.append(0)\n",
    "        mediumLong.append(0)\n",
    "        long.append(0)\n",
    "    elif(duration < 1245):\n",
    "        short.append(0)\n",
    "        mediumShort.append(1)\n",
    "        mediumLong.append(0)\n",
    "        long.append(0)\n",
    "    elif(duration < 1915):\n",
    "        short.append(0)\n",
    "        mediumShort.append(0)\n",
    "        mediumLong.append(1)\n",
    "        long.append(0)\n",
    "    else:\n",
    "        short.append(0)\n",
    "        mediumShort.append(0)\n",
    "        mediumLong.append(0)\n",
    "        long.append(1)\n",
    "train.insert(7,\"Short\", short)\n",
    "train.insert(8,\"Relatively Short\", mediumShort)\n",
    "train.insert(9,\"Relatively Long\", mediumLong)\n",
    "train.insert(10,\"Long\", long)\n",
    "\n",
    "X = train.drop(['Model','Start Date', 'End Date','Duration','Short','Relatively Short','Relatively Long','Long'], axis=1)\n",
    "Y = train.drop(['Nct_id','Summary','Title', 'Start Date', 'End Date','Model','Duration'], axis = 1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f674cd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Short  Relatively Short  Relatively Long  Long\n",
      "7189      0                 0                0     1\n",
      "3258      1                 0                0     0\n",
      "2997      1                 0                0     0\n",
      "7698      0                 0                1     0\n",
      "5614      0                 0                0     1\n",
      "           Nct_id                                            Summary  \\\n",
      "7189  NCT01731951   This pilot clinical trial studies how well im...   \n",
      "3258  NCT00412880   Open label, uncontrolled Phase II trial to as...   \n",
      "2997  NCT00362583   Primary objectives: - To confirm the efficacy...   \n",
      "7698  NCT02002312   To determine the efficacy of multiple doses L...   \n",
      "5614  NCT01079780   RATIONALE: Drugs used in chemotherapy, such a...   \n",
      "\n",
      "                                                  Title  \n",
      "7189  Imetelstat Sodium in Treating Participants Wit...  \n",
      "3258            BI 2536 Second Line Monotherapy in SCLC  \n",
      "2997  Efficacy and Safety of Intranasal Fentanyl in ...  \n",
      "7698  Phase II Study of Lutetium-177 Labeled Girentu...  \n",
      "5614  Irinotecan Hydrochloride and Cetuximab With or...  \n",
      "(6137, 3)\n",
      "(6137, 4)\n",
      "(1535, 3)\n",
      "(1535, 4)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.head()) # Prints the first 5 rows of the data\n",
    "print(X_train.head())\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc836d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e501fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bf512bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2df9dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be download for the lemmatization (converting to base form)\n",
    "def text_cleaner(dataframe_org):\n",
    "    dataframe = dataframe_org.copy()\n",
    "    columns = ['Summary', 'Title']\n",
    "    for col in columns:\n",
    "        dataframe[col] = dataframe[col].str.translate(str.maketrans(' ', ' ', string.punctuation)) # Remove punctuation\n",
    "        dataframe[col] = dataframe[col].str.translate(str.maketrans(' ', ' ', '\\n')) # Remove newlines\n",
    "        dataframe[col] =dataframe[col].str.translate(str.maketrans(' ', ' ', digits)) # Remove digits\n",
    "        dataframe[col] =dataframe[col].apply(lambda tweet: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',tweet)) # Split combined words\n",
    "        dataframe[col] =dataframe[col].str.lower() # Convert to lowercase\n",
    "        dataframe[col] =dataframe[col].str.split() # Split each sentence using delimiter\n",
    "    # This part is for converting to base form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sum_l=[]\n",
    "    tit_l = []\n",
    "    for y in tqdm(dataframe[columns[0]]): # tqdm is just a progress bar, an this loop only looks at summaries\n",
    "        sum_new=[]\n",
    "        for x in y: # Looks at words in every summary text\n",
    "            z=lemmatizer.lemmatize(x)\n",
    "            z=lemmatizer.lemmatize(z,'v') # The v specifies that it is in doubt of example a word is a noun or verb, it would consider it a verb.\n",
    "            sum_new.append(z)\n",
    "        y = sum_new\n",
    "        sum_l.append(y)\n",
    "    for w in tqdm(dataframe[columns[1]]): # Looks at titles\n",
    "        tit_new=[]\n",
    "        for x in w: # Every word in the titles\n",
    "            z=lemmatizer.lemmatize(x)\n",
    "            z=lemmatizer.lemmatize(z,'v')\n",
    "            tit_new.append(z)\n",
    "        w = tit_new\n",
    "        tit_l.append(w)\n",
    "    # This will join the words into strings as in the original data, just pre-processed and put into list\n",
    "    sum_l2 = []\n",
    "    for col in sum_l:\n",
    "        col = ' '.join(col)\n",
    "        sum_l2.append(col)\n",
    "    tit_l2 = []\n",
    "    for col in tit_l:\n",
    "        col = ' '.join(col)\n",
    "        tit_l2.append(col)\n",
    "    # Data obtained after Lemmatization is in array form, and is converted to Dataframe in the next step.\n",
    "    sum_data=pd.DataFrame(np.array(sum_l2), index=dataframe.index,columns={columns[0]})\n",
    "    tit_data=pd.DataFrame(np.array(tit_l2), index=dataframe.index,columns={columns[1]})\n",
    "    frames = [sum_data, tit_data]\n",
    "    merged = pd.concat(frames, axis=1)\n",
    "    return merged\n",
    "def create_tok(train_data, MAX_FEATURES):\n",
    "    clean_data = text_cleaner(train_data)\n",
    "    tokenizer_sum = text.Tokenizer(num_words=MAX_FEATURES) # Keep the 20.000 most frequent words\n",
    "    tokenizer_tit =  text.Tokenizer(num_words=MAX_FEATURES)\n",
    "    # Summary Text\n",
    "    summary_list = clean_data['Summary']\n",
    "    tokenizer_sum.fit_on_texts(list(summary_list)) # Builds the word index\n",
    "    #Title Text\n",
    "    title_list = clean_data['Title'] # Text from Title\n",
    "    tokenizer_tit.fit_on_texts(list(title_list))\n",
    "    return tokenizer_sum, tokenizer_tit\n",
    "def pre_process(dataframe, tokenizer, col, MAXLEN):\n",
    "    clean_data = text_cleaner(dataframe)\n",
    "    tokenized_list = tokenizer.texts_to_sequences(clean_data[col])\n",
    "    X = sequence.pad_sequences(tokenized_list, maxlen=MAXLEN)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bcc43ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6137/6137 [00:02<00:00, 2119.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6137/6137 [00:00<00:00, 16822.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6137/6137 [00:01<00:00, 3163.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6137/6137 [00:00<00:00, 16808.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6137/6137 [00:01<00:00, 3166.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6137/6137 [00:00<00:00, 16785.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   16   14   32]\n",
      " [   0    0    0 ... 7193  875   11]\n",
      " [   0    0    0 ... 3153 1297   95]\n",
      " ...\n",
      " [   0    0    0 ...  967   79  476]\n",
      " [   0    0    0 ...   97    3   10]\n",
      " [   0    0    0 ...   18    7 7192]]\n",
      "[[   0    0    0 ...    8  567  657]\n",
      " [   0    0    0 ...  175    1  548]\n",
      " [   0    0    0 ...  568  142 1496]\n",
      " ...\n",
      " [   0    0    0 ...    6   61    2]\n",
      " [   0    0    0 ...   20   16  103]\n",
      " [   0    0    0 ...  132   20 5614]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1535/1535 [00:00<00:00, 3168.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1535/1535 [00:00<00:00, 15988.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1535/1535 [00:00<00:00, 3144.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1535/1535 [00:00<00:00, 16680.89it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 20000 # Size of vocabluary\n",
    "MAXLEN = 220 # Size of each text sequence, you can tune this depending on the mean length of you text sequences\n",
    "tok_sum, tok_tit = create_tok(X_train,MAX_FEATURES )\n",
    "# The following are used for model.fit\n",
    "X_sum = pre_process(X_train, tok_sum, 'Summary', MAXLEN)\n",
    "X_tit = pre_process(X_train, tok_tit, 'Title', MAXLEN)\n",
    "print(X_sum)\n",
    "print(X_tit)\n",
    "# This is used for prediction\n",
    "X_sum_test = pre_process(X_test, tok_sum, 'Summary', MAXLEN)\n",
    "X_tit_test = pre_process(X_test, tok_tit, 'Title', MAXLEN)\n",
    "list_classes = [\"Short\", \"Relatively Short\", \"Relatively Long\", \"Long\"] # The 4 categories\n",
    "y = Y_train[list_classes].values\n",
    "# y_test is used for model.evaluate later on\n",
    "y_test = Y_test[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c8a0896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 220)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 220)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 440)          0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 440, 50)      1000000     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 50)           20200       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50)           0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           2550        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 50)          200         ['dropout_1[0][0]']              \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            204         ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,023,154\n",
      "Trainable params: 1,023,054\n",
      "Non-trainable params: 100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_con_model():\n",
    "    embed_size = 50 # How big  each word vector should be\n",
    "    inp_sum = Input(shape=(MAXLEN, ))\n",
    "    inp_title = Input(shape=(MAXLEN, ))\n",
    "    total_inp = concatenate([inp_sum, inp_title]) # Merge the 2 inputs\n",
    "    embed_layer = Embedding(MAX_FEATURES, embed_size)(total_inp)\n",
    "    lstm_layer = LSTM(50)(embed_layer)\n",
    "    layer1 = Dropout(0.1)(lstm_layer) # Regularization method, has the effect of reducing overfitting\n",
    "    layer2 = Dense(50, activation=\"relu\")(layer1) # The relu function can return very large values\n",
    "    layer3 =  Dropout(0.1)(layer2) # Again regularization\n",
    "    layer4 =BatchNormalization()(layer3) # Maintains the mean activation close to 0 and the activation standard deviation close to 1\n",
    "    layer5 = Dense(4, activation=\"softmax\")(layer4) # Only outputs values between 0 and 1, this is the final layer\n",
    "    model_con = Model(inputs=[inp_sum,inp_title], outputs=layer5)\n",
    "    model_con.compile(loss='categorical_crossentropy', # This is the loss function, and this type of function is used when solving categorical classification\n",
    "                    optimizer='rmsprop', # Algorithm that update network weights iterative based in training data\n",
    "                    metrics=['accuracy']) # This is our statistical measure\n",
    "    return model_con\n",
    "con_model = get_con_model()\n",
    "# Gets informations about the layers in the model, including output, input and number of parameters:\n",
    "con_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa66d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.38064, saving model to weights_base.hdf5\n",
      "173/173 - 13s - loss: 1.3741 - accuracy: 0.2901 - val_loss: 1.3806 - val_accuracy: 0.2687 - 13s/epoch - 74ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_loss improved from 1.38064 to 1.36502, saving model to weights_base.hdf5\n",
      "173/173 - 11s - loss: 1.3034 - accuracy: 0.3699 - val_loss: 1.3650 - val_accuracy: 0.2801 - 11s/epoch - 64ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_loss improved from 1.36502 to 1.35626, saving model to weights_base.hdf5\n",
      "173/173 - 11s - loss: 1.2284 - accuracy: 0.4367 - val_loss: 1.3563 - val_accuracy: 0.3208 - 11s/epoch - 64ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.35626\n",
      "173/173 - 11s - loss: 1.1295 - accuracy: 0.5010 - val_loss: 1.3725 - val_accuracy: 0.3143 - 11s/epoch - 63ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.35626\n",
      "173/173 - 11s - loss: 1.0334 - accuracy: 0.5684 - val_loss: 1.5863 - val_accuracy: 0.2834 - 11s/epoch - 64ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.35626\n",
      "173/173 - 11s - loss: 0.9489 - accuracy: 0.6089 - val_loss: 1.8461 - val_accuracy: 0.3046 - 11s/epoch - 64ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # number of samples that will be propagated through the network.\n",
    "epochs = 10 # Number of passes over the entire data set\n",
    "file_path=\"weights_base.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min') # Verbose means that it prints acc and loss\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3) \n",
    "#EarlyStopping should only be includede when tuning your model\n",
    "callbacks_list = [checkpoint, early]\n",
    "history = con_model.fit([X_sum, X_tit], y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list, verbose=2) # Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_model.load_weights(file_path)\n",
    "con_model.evaluate([X_sum_test, X_tit_test], y_test, verbose=2) # Returns loss value and the metric specified, so in this case, model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a865655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
