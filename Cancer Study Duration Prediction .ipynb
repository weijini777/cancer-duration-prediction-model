{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e451b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Linear algebra\n",
    "# Data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import xml.etree.ElementTree as ET # Reading xml files\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import plot_model\n",
    "from sklearn.manifold import TSNE\n",
    "# For Modelling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, preprocessing, callbacks, optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Add\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.metrics import categorical_accuracy\n",
    "# For Pre-processing\n",
    "import string\n",
    "from string import digits\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "# Other useful modules\n",
    "import h5py\n",
    "from statistics import mode\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f60c59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"NCT00000479\";\"\n",
      " The purpose of this study is to evaluate the effects of low-dose aspirin and vitamin E in primary prevention of cardiovascular disease and cancer in apparently healthy women. \";\"Women's Health Study (WHS): A Randomized Trial of Low-dose Aspirin and Vitamin E in the Primary Prevention of Cardiovascular Disease and Cancer\";\"Factorial Assignment\";\"September 1992\";\"March 2004\"\n"
     ]
    }
   ],
   "source": [
    "def csv_row(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    nct_text = \"\"\n",
    "    sum_text = \"\"\n",
    "    model_text = \"\"\n",
    "    ph_text = \"\"\n",
    "    title_text = \"\"\n",
    "    start_date = \"\"\n",
    "    completion_date = \"\"\n",
    "    # Only iterates through Phase 2 and 3 studies\n",
    "    for ph in root.iter('phase'):\n",
    "        ph_text = ph.text\n",
    "        if (ph_text == \"Phase 2\" or ph_text == \"Phase 3\" or ph_text == \"Phase 1\"):\n",
    "            #This bit finds all roots with nct_id which is a sub_root to id_info\n",
    "            for nct in root.findall('id_info'):\n",
    "                nctId_text = nct.find('nct_id').text\n",
    "                nct_text =nctId_text\n",
    "            # This bit finds the brief summary text\n",
    "            for s in root.findall('brief_summary'):\n",
    "                summary_text = s.find('textblock').text\n",
    "                sum_text= summary_text\n",
    "                sum_text = sum_text.replace('\\r\\n', '') # Replaces newline with a whitespace\n",
    "                sum_text = re.sub(' +',' ',sum_text) # Compresses multiple whitespaces to only one\n",
    "                #print(\"Summary Text:\", sum_text)\n",
    "            # Get's the official title for the study\n",
    "            for t in root.iter('brief_title'):\n",
    "                title_text = t.text\n",
    "            # This get's the type of intervention_model\n",
    "            for y in root.iter('intervention_model'):\n",
    "                model_text = y.text\n",
    "            if(root.find('start_date') == None):\n",
    "                return None \n",
    "            for s in root.iter('start_date'):\n",
    "                start_date = s.text\n",
    "            if(root.find('primary_completion_date') == None):\n",
    "                return None    \n",
    "            for c in root.iter('primary_completion_date'):\n",
    "                completion_date = c.text\n",
    "    total_text = \"\\\"\" + nct_text + \"\\\"\" + \";\" + \"\\\"\" + sum_text + \"\\\"\" + \";\"  + \"\\\"\" + title_text + \"\\\"\" + \";\"  +  \"\\\"\" + model_text + \"\\\"\"+ \";\"  + \"\\\"\" + start_date + \"\\\"\"+ \";\"  + \"\\\"\" + completion_date + \"\\\"\"\n",
    "    # This functions returns a text with Nct_Id, brief_summary, title and type of intervention model on the form we intended\n",
    "    return total_text\n",
    "print(csv_row(\"Downloads\\\\search_result (1)\\\\NCT00000479.xml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e81f372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = \"Downloads\\\\search_result (1)\"# Folders in directory where the all the xml files are placed\n",
    "with open('train_data.csv', 'w', encoding=\"utf-8\") as csvfile: \n",
    "    for root, dirs, files in os.walk(rdir):\n",
    "        for filename in files:\n",
    "            name = os.path.join(root, filename)\n",
    "            data = csv_row(name)\n",
    "            if(data):\n",
    "                csvfile.write(data) #Writes total_text into a row in to train_data.csv\n",
    "                csvfile.write(\"\\n\") # Skips to next line and do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d42c180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Nct_id                                            Summary  \\\n",
      "0     NCT00000479   The purpose of this study is to evaluate the ...   \n",
      "1     NCT00001277   Observational Phase: Patients whose parathyro...   \n",
      "4     NCT00001566   This is a single arm study. The tumor specime...   \n",
      "5     NCT00001637   Diseases such as leukemia, lymphoma, and mult...   \n",
      "6     NCT00001806   In 1997, the Genetics Department of the NCI M...   \n",
      "...           ...                                                ...   \n",
      "9982  NCT05502458   To investigate the effects of perioperative a...   \n",
      "9984  NCT05553808   This study is a sub-study of the master proto...   \n",
      "9985  NCT05591456   Accelerated hypofractionated 1 week post-mast...   \n",
      "9986  NCT05614518   The goal of this clinical trial was to assess...   \n",
      "9987  NCT05622357   The goal of this clinical trial is to investi...   \n",
      "\n",
      "                                                  Title  \\\n",
      "0     Women's Health Study (WHS): A Randomized Trial...   \n",
      "1              Studies of Elevated Parathyroid Activity   \n",
      "4     A Pilot Study of Autologous T-Cell Transplanta...   \n",
      "5     Immunosuppressive Preparation Followed by Bloo...   \n",
      "6       Methods in Education for Breast Cancer Genetics   \n",
      "...                                                 ...   \n",
      "9982  Effect of Propofol and Desflurane on Nucleic A...   \n",
      "9984  Platform Trial of Novel Regimens Versus Standa...   \n",
      "9985  Accelerated Hypofractionated 1 Week Radiothera...   \n",
      "9986  Diagnostic Performance and Safety of 18F-NaF-P...   \n",
      "9987  Prospective Study of Short Course Radiation Th...   \n",
      "\n",
      "                        Model  Duration          Start Date  \\\n",
      "0        Factorial Assignment      4200      September 1992   \n",
      "1         Parallel Assignment      9581   December 15, 1993   \n",
      "4     Single Group Assignment      4290       December 1996   \n",
      "5     Single Group Assignment      3954  September 29, 1997   \n",
      "6         Parallel Assignment      6810       April 6, 1999   \n",
      "...                       ...       ...                 ...   \n",
      "9982      Parallel Assignment       617    December 1, 2020   \n",
      "9984      Parallel Assignment       969    January 24, 2019   \n",
      "9985      Parallel Assignment       544     January 1, 2018   \n",
      "9986     Crossover Assignment       428      March 29, 2021   \n",
      "9987  Single Group Assignment      1002      March 18, 2018   \n",
      "\n",
      "                End Date  \n",
      "0             March 2004  \n",
      "1         March 11, 2020  \n",
      "4         September 2008  \n",
      "5          July 28, 2008  \n",
      "6       December 6, 2017  \n",
      "...                  ...  \n",
      "9982      August 8, 2022  \n",
      "9984  September 23, 2021  \n",
      "9985       June 30, 2019  \n",
      "9986        June 2, 2022  \n",
      "9987   December 20, 2020  \n",
      "\n",
      "[7685 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Earlier we saw that the returned text from our function was seperated by ';', so we use this as seperator when reading in the files\n",
    "df = pd.read_csv(\"train_data.csv\", sep=';', header=None,error_bad_lines=False, warn_bad_lines=False)\n",
    "# Give the data sets appropiate column names\n",
    "df.columns =  ['Nct_id', 'Summary', 'Title','Model','Start Date','End Date']\n",
    "# We drop all the observations containing NaN's (missing values)\n",
    "train = df.dropna()\n",
    "duration = []\n",
    "monthToNum = {\"January\": 1, \"February\":2, \"March\":3, \"April\":4, \"May\":5, \"June\":6, \"July\":7, \"August\":8, \"September\":9, \"October\":10, \"November\":11, \"December\":12}\n",
    "for i,row in train.iterrows():\n",
    "    if(\"\\r\\n\" in row['Summary']):\n",
    "        row['Summary'] = row['Summary'].replace(\"\\r\\n\", '')\n",
    "\n",
    "    sd = row['Start Date'].split(' ')\n",
    "    if(len(sd) == 2):\n",
    "        sdl = monthToNum[sd[0]]*30 + 365*int(sd[1]) + 1\n",
    "    else:\n",
    "        sdl = monthToNum[sd[0]]*30 + int(sd[1].replace(\",\", \"\")) + int(sd[2])*365\n",
    "    ed = row['End Date'].split(' ')\n",
    "    if(len(ed) == 2):\n",
    "        edl = monthToNum[ed[0]]*30 + 365*int(ed[1]) + 1\n",
    "    else:\n",
    "        edl = monthToNum[ed[0]]*30 + int(ed[1].replace(\",\", \"\")) + int(ed[2])*365\n",
    "    duration.append(edl - sdl)\n",
    "train.insert(4,\"Duration\", duration)\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f7847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Duration  Short  Relatively Short  Relatively Long  Long\n",
      "0.25     790.0    0.0               0.0              0.0   0.0\n",
      "0.50    1245.0    0.0               0.0              0.0   0.0\n",
      "0.75    1885.0    1.0               0.0              1.0   0.0\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.quantile([0.25,0.5,0.75]))\n",
    "#one hot encoding for the duration results\n",
    "short = []\n",
    "mediumShort = []\n",
    "mediumLong = []\n",
    "long = []\n",
    "\n",
    "for i,row in train.iterrows():\n",
    "    duration = row['Duration']\n",
    "    if(duration < 790):\n",
    "        short.append(1)\n",
    "        mediumShort.append(0)\n",
    "        mediumLong.append(0)\n",
    "        long.append(0)\n",
    "    elif(duration < 1245):\n",
    "        short.append(0)\n",
    "        mediumShort.append(1)\n",
    "        mediumLong.append(0)\n",
    "        long.append(0)\n",
    "    elif(duration < 1885):\n",
    "        short.append(0)\n",
    "        mediumShort.append(0)\n",
    "        mediumLong.append(1)\n",
    "        long.append(0)\n",
    "    else:\n",
    "        short.append(0)\n",
    "        mediumShort.append(0)\n",
    "        mediumLong.append(0)\n",
    "        long.append(1)\n",
    "train.insert(7,\"Short\", short)\n",
    "train.insert(8,\"Relatively Short\", mediumShort)\n",
    "train.insert(9,\"Relatively Long\", mediumLong)\n",
    "train.insert(10,\"Long\", long)\n",
    "\n",
    "X = train.drop(['Model','Start Date', 'End Date','Duration','Short','Relatively Short','Relatively Long','Long'], axis=1)\n",
    "Y = train.drop(['Nct_id','Summary','Title', 'Start Date', 'End Date','Model','Duration'], axis = 1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "effd3e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Short  Relatively Short  Relatively Long  Long\n",
      "5181      0                 0                1     0\n",
      "9793      0                 1                0     0\n",
      "1381      0                 0                1     0\n",
      "7614      1                 0                0     0\n",
      "8574      0                 0                0     1\n",
      "           Nct_id                                            Summary  \\\n",
      "5181  NCT00938470   This randomized phase II trial studies how we...   \n",
      "9793  NCT04033419   Purpose: To conduct a one-arm phase II trial ...   \n",
      "1381  NCT00070148   RATIONALE: Oxandrolone and megestrol may help...   \n",
      "7614  NCT01954745   This research study is evaluating a drug call...   \n",
      "8574  NCT02551991   This is an open-label, phase 2 non-comparativ...   \n",
      "\n",
      "                                                  Title  \n",
      "5181  Docetaxel, Oxaliplatin, Capecitabine, Fluorour...  \n",
      "9793  Memantine for Prevention of Cognitive Decline ...  \n",
      "1381  Oxandrolone Compared With Megestrol in Prevent...  \n",
      "7614  Cabozantinib (XL-184) Monotherapy for Advanced...  \n",
      "8574  Study of Nanoliposomal Irinotecan (Nal-IRI)-Co...  \n",
      "(6148, 3)\n",
      "(6148, 4)\n",
      "(1537, 3)\n",
      "(1537, 4)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.head()) # Prints the first 5 rows of the data\n",
    "print(X_train.head())\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74d61ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef5f6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be download for the lemmatization (converting to base form)\n",
    "def text_cleaner(dataframe_org):\n",
    "    dataframe = dataframe_org.copy()\n",
    "    columns = ['Summary', 'Title']\n",
    "    for col in columns:\n",
    "        dataframe[col] = dataframe[col].str.translate(str.maketrans(' ', ' ', string.punctuation)) # Remove punctuation\n",
    "        dataframe[col] = dataframe[col].str.translate(str.maketrans(' ', ' ', '\\n')) # Remove newlines\n",
    "        dataframe[col] =dataframe[col].str.translate(str.maketrans(' ', ' ', digits)) # Remove digits\n",
    "        dataframe[col] =dataframe[col].apply(lambda tweet: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',tweet)) # Split combined words\n",
    "        dataframe[col] =dataframe[col].str.lower() # Convert to lowercase\n",
    "        dataframe[col] =dataframe[col].str.split() # Split each sentence using delimiter\n",
    "    # This part is for converting to base form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sum_l=[]\n",
    "    tit_l = []\n",
    "    for y in tqdm(dataframe[columns[0]]): # tqdm is just a progress bar, an this loop only looks at summaries\n",
    "        sum_new=[]\n",
    "        for x in y: # Looks at words in every summary text\n",
    "            z=lemmatizer.lemmatize(x)\n",
    "            z=lemmatizer.lemmatize(z,'v') # The v specifies that it is in doubt of example a word is a noun or verb, it would consider it a verb.\n",
    "            sum_new.append(z)\n",
    "        y = sum_new\n",
    "        sum_l.append(y)\n",
    "    for w in tqdm(dataframe[columns[1]]): # Looks at titles\n",
    "        tit_new=[]\n",
    "        for x in w: # Every word in the titles\n",
    "            z=lemmatizer.lemmatize(x)\n",
    "            z=lemmatizer.lemmatize(z,'v')\n",
    "            tit_new.append(z)\n",
    "        w = tit_new\n",
    "        tit_l.append(w)\n",
    "    # This will join the words into strings as in the original data, just pre-processed and put into list\n",
    "    sum_l2 = []\n",
    "    for col in sum_l:\n",
    "        col = ' '.join(col)\n",
    "        sum_l2.append(col)\n",
    "    tit_l2 = []\n",
    "    for col in tit_l:\n",
    "        col = ' '.join(col)\n",
    "        tit_l2.append(col)\n",
    "    # Data obtained after Lemmatization is in array form, and is converted to Dataframe in the next step.\n",
    "    sum_data=pd.DataFrame(np.array(sum_l2), index=dataframe.index,columns={columns[0]})\n",
    "    tit_data=pd.DataFrame(np.array(tit_l2), index=dataframe.index,columns={columns[1]})\n",
    "    frames = [sum_data, tit_data]\n",
    "    merged = pd.concat(frames, axis=1)\n",
    "    return merged\n",
    "def create_tok(train_data, MAX_FEATURES):\n",
    "    clean_data = text_cleaner(train_data)\n",
    "    tokenizer_sum = text.Tokenizer(num_words=MAX_FEATURES) # Keep the 20.000 most frequent words\n",
    "    tokenizer_tit =  text.Tokenizer(num_words=MAX_FEATURES)\n",
    "    # Summary Text\n",
    "    summary_list = clean_data['Summary']\n",
    "    tokenizer_sum.fit_on_texts(list(summary_list)) # Builds the word index\n",
    "    #Title Text\n",
    "    title_list = clean_data['Title'] # Text from Title\n",
    "    tokenizer_tit.fit_on_texts(list(title_list))\n",
    "    return tokenizer_sum, tokenizer_tit\n",
    "def pre_process(dataframe, tokenizer, col, MAXLEN):\n",
    "    clean_data = text_cleaner(dataframe)\n",
    "    tokenized_list = tokenizer.texts_to_sequences(clean_data[col])\n",
    "    X = sequence.pad_sequences(tokenized_list, maxlen=MAXLEN)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f4a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6148/6148 [00:03<00:00, 2012.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6148/6148 [00:00<00:00, 16403.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6148/6148 [00:02<00:00, 3023.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6148/6148 [00:00<00:00, 15562.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6148/6148 [00:02<00:00, 3003.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6148/6148 [00:00<00:00, 16497.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    4    3  213]\n",
      " [   0    0    0 ...  324    8 5455]\n",
      " [   0    0    0 ...   16  437   15]\n",
      " ...\n",
      " [   0    0    0 ...   41   35  169]\n",
      " [   0    0    0 ...   94    6  157]\n",
      " [   0    0    0 ...   30  441  112]]\n",
      "[[  0   0   0 ...   8 169 221]\n",
      " [  0   0   0 ...   3  13   2]\n",
      " [  0   0   0 ...  17  11   2]\n",
      " ...\n",
      " [  0   0   0 ...  12  13   2]\n",
      " [  0   0   0 ...  66  19   2]\n",
      " [  0   0   0 ... 260   4  40]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1537/1537 [00:00<00:00, 2800.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1537/1537 [00:00<00:00, 17137.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1537/1537 [00:00<00:00, 2836.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1537/1537 [00:00<00:00, 15989.06it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 20000 # Size of vocabluary\n",
    "MAXLEN = 220 # Size of each text sequence, you can tune this depending on the mean length of you text sequences\n",
    "tok_sum, tok_tit = create_tok(X_train,MAX_FEATURES )\n",
    "# The following are used for model.fit\n",
    "X_sum = pre_process(X_train, tok_sum, 'Summary', MAXLEN)\n",
    "X_tit = pre_process(X_train, tok_tit, 'Title', MAXLEN)\n",
    "print(X_sum)\n",
    "print(X_tit)\n",
    "# This is used for prediction\n",
    "X_sum_test = pre_process(X_test, tok_sum, 'Summary', MAXLEN)\n",
    "X_tit_test = pre_process(X_test, tok_tit, 'Title', MAXLEN)\n",
    "list_classes = [\"Short\", \"Relatively Short\", \"Relatively Long\", \"Long\"] # The 4 categories\n",
    "y = Y_train[list_classes].values\n",
    "# y_test is used for model.evaluate later on\n",
    "y_test = Y_test[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a03208db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 220)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 220)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 440)          0           ['input_5[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 440, 50)      1000000     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 50)           20200       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 50)           0           ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50)           2550        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 50)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 50)          200         ['dropout_5[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4)            204         ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,023,154\n",
      "Trainable params: 1,023,054\n",
      "Non-trainable params: 100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_con_model():\n",
    "    embed_size = 50 # How big  each word vector should be\n",
    "    inp_sum = Input(shape=(MAXLEN, ))\n",
    "    inp_title = Input(shape=(MAXLEN, ))\n",
    "    total_inp = concatenate([inp_sum, inp_title]) # Merge the 2 inputs\n",
    "    embed_layer = Embedding(MAX_FEATURES, embed_size)(total_inp)\n",
    "    lstm_layer = LSTM(50)(embed_layer)\n",
    "    layer1 = Dropout(0.1)(lstm_layer) # Regularization method, has the effect of reducing overfitting\n",
    "    layer2 = Dense(50, activation=\"relu\")(layer1) # The relu function can return very large values\n",
    "    layer3 =  Dropout(0.1)(layer2) # Again regularization\n",
    "    layer4 =BatchNormalization()(layer3) # Maintains the mean activation close to 0 and the activation standard deviation close to 1\n",
    "    layer5 = Dense(4, activation=\"softmax\")(layer4) # Only outputs values between 0 and 1, this is the final layer\n",
    "    model_con = Model(inputs=[inp_sum,inp_title], outputs=layer5)\n",
    "    model_con.compile(loss='categorical_crossentropy', # This is the loss function, and this type of function is used when solving categorical classification\n",
    "                    optimizer='rmsprop', # Algorithm that update network weights iterative based in training data\n",
    "                    metrics=['accuracy']) # This is our statistical measure\n",
    "    return model_con\n",
    "con_model = get_con_model()\n",
    "# Gets informations about the layers in the model, including output, input and number of parameters:\n",
    "con_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b053f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.37788, saving model to weights_base.hdf5\n",
      "173/173 - 16s - loss: 1.3757 - accuracy: 0.2933 - val_loss: 1.3779 - val_accuracy: 0.2780 - 16s/epoch - 92ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_loss improved from 1.37788 to 1.36663, saving model to weights_base.hdf5\n",
      "173/173 - 14s - loss: 1.3062 - accuracy: 0.3712 - val_loss: 1.3666 - val_accuracy: 0.3366 - 14s/epoch - 83ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_loss improved from 1.36663 to 1.34116, saving model to weights_base.hdf5\n",
      "173/173 - 15s - loss: 1.2189 - accuracy: 0.4303 - val_loss: 1.3412 - val_accuracy: 0.3447 - 15s/epoch - 88ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.34116\n",
      "173/173 - 16s - loss: 1.1276 - accuracy: 0.4994 - val_loss: 1.4342 - val_accuracy: 0.3366 - 16s/epoch - 93ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.34116\n",
      "173/173 - 16s - loss: 1.0247 - accuracy: 0.5664 - val_loss: 1.4719 - val_accuracy: 0.3415 - 16s/epoch - 90ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.34116\n",
      "173/173 - 16s - loss: 0.9441 - accuracy: 0.6080 - val_loss: 1.5705 - val_accuracy: 0.3220 - 16s/epoch - 93ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # number of samples that will be propagated through the network.\n",
    "epochs = 10 # Number of passes over the entire data set\n",
    "file_path=\"weights_base.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min') # Verbose means that it prints acc and loss\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3) \n",
    "#EarlyStopping should only be includede when tuning your model\n",
    "callbacks_list = [checkpoint, early]\n",
    "history = con_model.fit([X_sum, X_tit], y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list, verbose=2) # Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da92fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
